---
layout: post
title: "Docker"
category: programming
date: 2025-07-28
---

<sub> It's all about the containers that we ship and run <sub>

1. TOC
{:toc}

---

# Containers

We can package the application into **container** - _standardized_ executable component (that means that it can run on any machine), which holds our code along with all the dependencies (environment) needed to run the code!

An immutable blueprint for a container is called an **image**.

**Containerization** - a process of packaging an application into a container.

Containers aren’t virtual machines, the main difference is that containers _share the host’s kernel_ instead of simulating an "entire computer". They give us a sandboxed environment with isolated processes, files and network interfaces, without the need to boot up a whole OS every time.

Usually the images we base our containers on are stripped‑down user‑space environments, carrying only what’s needed for a specific task. Of course, one could install a full Ubuntu in a container, but preferably we should pick a minimal image (think Alpine, Debian slim or nvidia/cuda for GPU work) and add just the libraries/tools they need.

{% include info.html text="VM creates a virtual CPU, virtual memory, a virtual network card and so on. Then Ubuntu’s own kernel has to initialize all of that, discover the “hardware”, load drivers and finally start services."%}

# Docker

**Docker** is a TOOL that allows us to build and run containers, save them into templates. When we write a Dockerfile, we’re telling Docker something like "_start from this base image, install Python and PyTorch, copy in my training code, set the entry point to python train.py_" so that every time we build that file, we get an identical image that we can spin up anywhere.

For example:

```
docker run --gpus all my‑dl‑image python train.py
```

Docker takes our image *my-dl-image*, mounts a writable layer on top, sets up the _linux namespaces_ and _cgroups_, hooks up the GPU devices and network, and then kicks off `python train.py`

Let's examine an instance of a *my-dl-image* Dockerfile layer-by-layer:
```
FROM nvidia/cuda:12.1-cudnn8-runtime-ubuntu22.04
WORKDIR /workspace
COPY requirements.txt .
RUN apt-get update && apt-get install -y python3-pip && \
    pip3 install -r requirements.txt
COPY train.py .
ENTRYPOINT ["python3", "train.py"]
```
which we can build like:
```
docker build -t my-dl-image .
```

When we are ready to train a DL model, we use a container based on NVIDIA’s CUDA image so we would get the right GPU drivers, CUDA toolkit and cuDNN. Inside that container we `pip‑install torch`, copy over some dataset and `run python train.py`.

# Reading

1. https://levelup.gitconnected.com/docker-beginner-to-expert-tutorial-68555aa3e544
2. https://docs.docker.com/get-started/

<sub> The official Docker documentation is just brilliant, especially if we are comparing it to the Google Maps API. <sub>
